# Data Modeling with Postgres

## **Overview**
This is to simulate a situation that the analytics team from a music startup, Sparkify, is interested in understanding their user's insight from the data they have been collecting, such as songs and user activity.
However, they don't have an easy way to query their data, which resides in a directory of JSON logs on user activity on the app, as well as a directory with JSON metadata on the songs in their app.
So, this project aims to:
- To create a database schema defining fact and dimension tables for a star schema for a particular analytic focus.
- To create an ETL pipeline that transfers data from files in two local directories into these tables in Postgres using Python and SQL.

## **Dataset**

## **Song Data**
Song dataset is a subset of real data from the [Million Song Dataset](http://millionsongdataset.com/). The data is in JSON format and contains metadata about a song and the artist of that song.

Sample Song Data:
```
{"num_songs":1,"artist_id":"ARD7TVE1187B99BFB1","artist_latitude":null,"artist_longitude":null,"artist_location":"California - LA","artist_name":"Casual","song_id":"SOMZWCG12A8C13C480","title":"I Didn't Mean To","duration":218.93179,"year":0}
```
## **Log Data**
Log dataset is in JSON format generated by this [event simulator](https://github.com/Interana/eventsim) based on the songs in the dataset above.

Sample Log Data:
```
{"artist":"Des'ree","auth":"Logged In","firstName":"Kaylee","gender":"F","itemInSession":1,"lastName":"Summers","length":246.30812,"level":"free","location":"Phoenix-Mesa-Scottsdale, AZ","method":"PUT","page":"NextSong","registration":1540344794796.0,"sessionId":139,"song":"You Gotta Be","status":200,"ts":1541106106796,"userAgent":"\"Mozilla\/5.0 (Windows NT 6.1; WOW64) AppleWebKit\/537.36 (KHTML, like Gecko) Chrome\/35.0.1916.153 Safari\/537.36\"","userId":"8"}
```

## **Database Schema**
In this project, the database schema is based on the star schema, which includes Fact table and Dimension tables.
### Fact table:
- `songplays` - records in log data associated with song plays i.e. records with page `NextSong`
  
  ```
  songplay_id, start_time, user_id, level, song_id, artist_id, session_id, location, user_agent
  ```
  

### Dimension tables:
- `users` - users in the app

  ```
  user_id, first_name, last_name, gender, level
  ```
  
- `songs` - songs in music database
  
  ```
  song_id, title, artist_id, year, duration
  ```
  
- `artists` - artists in music database
  
  ```
  artist_id, name, location, latitude, longitude
  ```
  
- `time` - timestamps of records in <strong>songplays</strong> broken down into specific units
  
  ```
  start_time, hour, day, week, month, year, weekday
  ```
  
## **Project Setup**
1. After cloning and navigating to the root directory for the project, make sure your system has `Python3` and `pip3` installed already. Check in Terminal by
```
$ which python3
$ which pip3
```
2. Install virtualenv using pip
```
$ pip3 install virtualenv
```
3. From the project directory, create a new virtual environment for ths project and then activate.
```
$ virtualenv venv
$ source venv/bin/activate
```
4. Install project dependencies
```
$ pip3 install -r requirement.txt
```
5. In the project root directory, create a `.env` file. In the file, write:
```
DB_NAME='{your default database name}'
DB_USERNAME='{your default database username}'
DB_PASSWORD='{your default database password}'
```

## **How to run**
1. Run `create_tables.py` first to create database connection and create empty fact and dimension tables
```
$ python3 create_tables.py
```
2. Run `etl.py` to perform ETL process and load all song and log data into the database tables.
```
$ python3 etl.py
```

## **Project Author**
- Author: Thitiwat Watanajaturaporn
- Note: this project is part of Udacity's Data Engineering Nanodegree Program.
